\documentclass[sigplan, 10pt]{acmart}
\usepackage[utf8]{inputenc}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\settopmatter{printfolios=true}
\begin{document}

\author{Romain Vaillant}
\email{romain.vaillant@scality.com}

\title{ElmerFS: Building a file system using CRDTs}

\begin{abstract}
\end{abstract}

\maketitle


\section{Introduction}


File system is one of the most approachable user interface to organize
and access data that we have tody. This paper address the problem
of highly available shared and geo-replicated distributed hierarchical
file systems (FS).

This is an old problem, the Andrew File System, a pioneer in distributed file
systems, dates back from 1982 and still receive updates today (refs).
It is based on a client-server architecture enabling file shared and
replicated read-only content distribution.
Firsts versions were able to scale up to hundreds of concurrent users (ref).

Bell Labs released a distributed operating system (OS) in 1992 (ref).
Based on the 9P protocol, this OS represents almost every resources and
service interfaces as files. Remote access and control is done with
the standard FS operations: mount, reads and writes.

Recently, cloud based file systems have appeared, focusing on ease of use.
Google Drive is a file hosting service that allows user to store, share and
synchronize files on multiple devices. It supports concurrent access across
the world and integrate with other services such as Google Doc or Google Slide
for collaborative work. Dropbox (ref) offer the same kind of services.

WeTransfer (ref), a file sharing service, can rely on a simpler abstractions,
object stores. They allow to persist often immutable blob data identified by a numeric key.
They don't provide a strict hierarchy and support a limited set of
operations: put, get and delete. They are easier to scale and have become
prevalent in cloud services.

However, many existing systems are either unavailable or incorrect, or both,
under some critical operations, and do not scale well to an enterprise
workload.

Distributed systems are notoriously hard to build and harder to
build right. Here, we explore the path of building a distributed file
system that is geos-distributed, highly resilient and scalable by using
\textbf{Conflict Free Replicated Data Types} (CRDTs), i.e. replicated
data structures that don't need any coordination. We leverage
\textbf{AntidoteDB}, a distributed database based
on CRDTs. Our implementation uses the rust programming language,
an efficient language which enforces memory and
thread safety statically.

Our experience shows that is is possible to build a file system, spanning multiple
data-centers, using CRDTs, that remains correct under extreme
failure and network conditions, while supports a heavy concurrent user
workloads with an intuitive user interface.

\section{Designing a file system}

\subsection{What a basic file system requires}

Our requirement is to follow the POSIX standard.  We implement
a user-space file system using the FUSE protocol (ref).

A file system supports structural operations operations (to manipulate the file system
structure, permissions and retrieving some information) and content operations
(read/write arbitrary amount of data to a given file).

We use the following terms:

\begin{itemize}
    \item \textbf{A inode}: Is the internal representation of a filesystem
    structure object (a file or a directory or a symbolic link).
    \item \textbf{A link}: A reference to some inode. An inode that is not
    linked may be delete
    \item \textbf{An ino}: A unique id used to reference an inode.
    \item \textbf{A directory entry}: The named and visible representation
    of a link. When a name is used to reference an inode,
    the system resolve internally the name into an ino.
\end{itemize}

The key structure of a file system is its tree hierarchy, the most common
properties that we require are:

\begin{enumerate}
    \item Nodes in this tree are links to inodes. A directory is a container
    of those links.
    \item Directories can have at most one existing link.
    \item All nodes are always reachable from the root directory.
    \item Cycles are possible only through symbolic links.
\end{enumerate}

The file system must also be resilient and concurrent and
have a behavior close to what an user might see in a local filesystem
supporting multiple users.

We whish to support enterprise workloads with concurrent user interactions. Our
expectation are that the application should be able to run up to 5
geo-distanced clusters with a minimum of 5 nodes per cluster.

Each node should be able to provide enough read write performance
for writing and reading documents and images with up to 100 concurrent users.

No focus have been done on I/O performances, but to adhere to this
workload we require to stay in the range of 1mBytes/s for writes
and more than 10mBytes/s for reads. We don't expect any limit on the file size
apart from the underlying storage capacity.

\subsection{Assumptions and objectives}

Our main objective is to implements a file system based on CRDTs. CRDT
ensure Strong Eventual Consistency (SEC) (ref) but do
not ensure that the application invariants remains correct. The challenge
is to keep those invariant always correct under any sequence of
operation.

We want to leverage CRDTs to develop a file system that is always available
and that provides good response times whatever the network conditions.
It must support active-active configurations (i.e. two geographically distant
clusters can issue read, write and structural operation at the same time without
coordination with each other).

Given the requirements, we want to keep the behavior of the file system as
close as possible to a local file system.

In summary, we want the following properties:

\begin{itemize}
    \item \textbf{Truly concurrent operations}: One way to
    handle concurrency is to serialize operation applied on the relevant objects.
    CRDTs avoids this and allow true concurrency without the need for a consensus.
    \item \textbf{Atomic operation}: POSIX impose that every FS operation
    is atomic.
    \item \textbf{Always available}: CRDT types allows to design system that
    are always available even under network extreme conditions.
    \item \textbf{Active-Active}: Several replica accept
    operations (structural and updates) concurrently and propagate them
    from one another, even after long delays.
    \item \textbf{Least surprise}: We don't want to perform too many actions
    or changes besides what the ones the users does. We want to
    avoid rollbacks or drastic structural changes behind the scene to stay close
    to a local file system behavior.
\end{itemize}

\section{Concurrency in a File System}

From a high level point of view, AntidoteDB is a key-value store, where each
value is a CRDT. AntidoteDB provides us a library needed CRDT types. This means
that the designer must map structures to the appropriate CRDT types and ensure
that he can express the application's operations with the set of operation
offered by the chosen CRDT types.

First, let's consider FS operations and their characteristics under an
environnement without coordination.

\subsection{Inode creation}

All operations that creates inodes
(\textit{mkdir} for directories and \textit{mknod} for files and symbolic links)
needs to generate an unique id.

This requires to ensure that no replicas can ever generate the same id even
if they can't talk to each other. This problems is often solved with the use
of UUIDs, a 16 bytes number, which have a large enough domain such
that the probability to generate a duplicate is practically null.
However, in POSIX, the requirement for this id is to be a 8 bytes number.

\subsection{Named links}

Operations that create or move links (\textit{rename}, \textit{link}
and creation operations) need to consider the case where two users
concurrently want to create a link with an identical name
inside a same directory.

In local file systems or distributed with coordination,
this problem does not exists as those operation are serialized. Serializing
those operation by the use of distributed locking is an option, but that would
lessen the advantages of CRDTs as almost all structural operations would need
coordination.

Additional problems arise if we decide to accept temporary conflicts. In such
cases, we have to inform the user of a conflict in the POSIX interface
which wasn't designed for this kind of situations. Care needs also to be taken
to not impact the current workload of one user from external structural
updates.

\subsection{Preserving the FS tree invariants}

By its nature \textit{rename} is an exclusive operation.
As soon as a link is moved, subsequent moves on the same link should fail.
When we decide to not serialize, this operation can create cycles
or unexpected additional links.

For example, without coordination, two users trying to rename
two directory into each other might make each directory be the parent of the other.
Once this situation arise, the concerned directories aren't reachable
from the root directory anymore.

The user can't solve the problem by himself and the application must
decide what must be done. Many distributed file system fails
to handle this situation, leaving the user with an unrecoverable error.

When \textit{rename} is used concurrently on the same source link to a different
destination, if both operations are accepted, it will create one additional
link of the inode. Additionally, for directories, the POSIX interface requires
that directories can only have one unique parent at all time.

\subsection{Deletion of inodes}

The \textit{unlink} operation, which removes links, poses problems
in the presence of concurrent operations that can create links.

Consider the simple case where one user \textit{unlink} a file while another
one \textit{link}s it. The inode's state stays correct when the
application will process the link operation from another replicas.

Recycling the inode (its unique id for example) also becomes a problems.
We cannot know when all replicas that once saw this inode
will converge to a state where this inode is not reachable anymore.

Similar issues concerning permissions can be observed. Usually coordination is
needed to ensure that permission change are honored by every replicas at
the time at which they are applied.

\section{Building blocks}

\subsection{Atomic Operations}

Each operation in ElmerFS is implemented as an AntidoteDB transaction.
They are interactive and are across multiple objects with causally consistent
reads an writes. Atomicity of our FS operations comes "for free" by using
this functionality and we designed ElmerFS as a layer between the POSIX
interface and the AntidoteDB.

\subsection{Composing CRDTs}

We split ElmerFS state into four mains entities: inodes,
blocks, directories entries.

\textbf{Inodes} stores all POSIX metadata information.
We represents this entity using a Remove Win Map (RWMap). \linebreak RWMaps favor remove operations in face
of concurrent adds.

This convergence property is crucial. Most operations only update a subset of
the inode attributes. In case of a concurrent \textit{unlink},
if no link remains, we explicitly remove all fields. Concurrent partial updates
coming from the other FS operations will be dropped.

\textbf{Blocks} are fixed size Last Writer Win register which can store
arbitrary content. They serve as storing file content and
symlink target paths. the application addresses the blocks implicatively when it receives
a write or read requests.

The advantage of this representation is that we do not
need to store an index of all existing blocks of a file.
The drawbacks are is that if we delete an inode,
we cannot explicitly delete an infinite amount of blocks, potentially leaving
blocks that are never freed.

\textbf{Directory Entries} are represented as Remove Win set (RWSet). Simplest directory
entries are pairs of an ino and a name.
For the same reasons as stated above, the remove win convergence property
is necessary. A set is better suited for directory entries than a map.
When we handles name conflicts we don't want to ever have two entries with
the same name and ino.


% \subsection{A common base}

% We won't describe in details how ElmerFS identify its entities,
% it simply computes a composite key from a kind
% (inode, block, directory, ...) and an ino.

% The ino generation is also very simple. Each instance reserves a predetermined
% amount of ino beforehand and keeps them in memory. To ensure that multiple
% instances do not produce the same ino, we have to use a lock. A counter CRDT
% does not work for this (we have to read the value, not just incrementing it). In fact
% this is the only place where a mandatory lock is used.
% A sequence CRDT (\textbf{TODO: which one}) might be a solution but aren't available in
% AntidoteDB.

% A consequence of this approach is that inos are never recycled. We can never
% be certain when a CRDT will never be used again as we allow divergence for an
% not constrained amount of time. However, for this particular case, 64bit is a
% large enough domain for this problem to be problematic for most common use
% cases.

% Our inode contains all metadata information that are common in POSIX:
% permissions, owner, change, modification and access
% time. As described above, we represent them using a RWMap.
% We made the decision not to store data information of the file and the
% symbolic link path alongside the metadata.

% As only inodes represent the existence of entities, this separation leads to
% potential leaks if some inode is deleted in one site and written to in another.
% However, it is more a limitation of the API of AntidoteDB than CRDTs which
% does not support partial read or update.

% \subsection{The heart of a file system interface}

% To represent the tree hierarchy, it is easy to see that just applying CRDTs merging semantics
% without care can easily break the tree constraints.
% There is active research (ref?) to provide a Tree CRDT with add, remove and move operations.
% In ElmerFS we didn't implement one of such CRDT that would have ease our
% development. We explored the path of only using RWMaps.

% Even though we still have found any simple solution to prevent cycle apart from
% keeping track of all rename operations and cancelling the
% one in conflict in a deterministic way, we still manage to handle
% most of the concurrent tree operations gracefully without distributed locks.
% This solution isn't perfect, our choice to prevent cycles would be costly at
% the application level.

% \section{Embracing the CRDT merging semantics}

% To handle most conflicting operations with only common
% CRDTs, ElmerFS introduces the concept of an owner of each operation and
% each owner has its own view of the current file system.

% \subsection{Some problematic cases}

% First, let's consider some problematic cases of the rename operation.
% On a POSIX file system, a rename operation is expected to be exclusive
% with respect to the renamed inodes.

% % confusing
% In a concurrent scenario, the first problematic situation
% arises when two users try to rename distinct inodes to the same target location with the same name.
% In that case, CRDT maps merging semantics will only keep one operation depending
% in which order they arrived. In all cases one link will be lost and the inode
% might become unreachable if the was the only link to that was referencing it.

% Moreover, those name conflicts can also happen with all operations that
% can create links: link, mkdir, mknod\dots

% The second situation happens when a unique reference is renamed
% by two users at the same time to two different locations. The standard behavior
% is that one of the operation will fail. This poses two new problems: In a completely
% asynchronous situation, how do we ensure that the state converge something that
% is meaningful to the user and how the reference count can be handled
% with an operation that semantically does not affect it ?

% \subsection{Keeping conflicts where they make sense}

% In ElmerFS, we solve these problems by using an unique value that we call the
% ViewID. This unique ID is used to identify the origin of the operations and to
% use the merge semantic of map and set CRDTs to our advantage. Apart from
% being unique, there is no particular requirement for this ID. In ElmerFS we used
% the file system user id (uid) assuming that a user might not be
% issuing operations from two different processes.
% Another solution would have been to tie a unique id to each process
% but we find that the mapping between the user and its actions is more
% straightforward in the former solution.

% In our application we use a RWSet CRDT to represent directories.
% We depart from standard file system by not only storing a pair of ino
% and name to represent a link but also by storing the ViewID alongside this
% original pair.

% If we take the previous concurrent case where two users tries to rename two
% inode to the same location, with the ViewID associated with each name, we
% ensure that we either don't have a conflict (no tuple in the set
% share a same name entry) or that we keep all succeeded operations
% in our application state: there might be two identical name in the set, but they
% will have a different ViewID.

% By adding uniqueness to our directory entries, we allow precise conflicts
% to bubble up to the application logic when they make sense.
% When two user try to create a new file inside a same directory, we don't
% really want one operation to fail afterward. As soon as the user created the
% file and started writing it, it is crucial for the file system to keep this
% file. Conflicts here allow us to choose how to present and how to manage
% conflict without designing a specific CRDTs.

% \subsection{Linking and reference counting}

% By using the same mechanism we can also keep track of the number of reference
% of each inode where a counter CRDT would obviously fall short. One can imagine two users
% renaming the same file to a different location. In ElmerFS, we accept this kind
% of operation but it means that this operation is similar to creating
% an additional link.
% We use a set CRDT to store all links of an inode.
% Inside a transaction,
% we issue a remove operation of the previous link and an add operation of the new link. The remove operation will contains the exact tuple matched
% (with the ViewID that created the link), When the CRDT converges, it receives
% two identical remove operations, and two distinct add operations, leading to the right reference count.

% Directories have the additional requirements that they can only have one unique
% parent in the file system. To preserve this invariant, we also use a register
% with last writer win semantics, to decide which reference is valid. Note that
% we could also allow multiple link of a directories,
% this is only a limitation of the POSIX interface.

% \subsection{Interfacing with the user}

% Interfacing with the user in case of non standard FS behaviors requires care.
% While we solved some issues with the renames and links operation, we still
% need to show to the user that the conflict exists and that it needs to be
% resolved. Since in a FS the user is expected to reference entities by name,
% we kept this idiom to resolve conflict.

% In ElmerFS, a name is either a shorthand or a fully qualified (FQN).
% What we call a FQN is a standard POSIX name (the prefix) and
% the ViewID delimited by a special character.
% For example, in ElmerFS we use the uid for convenience and
% we allow user to specify the username instead of the id directly:
% the FQN of \textit{file1} for the user
% \textit{bob} is \textit{file1:bob}. When only the filename is specified, we
% implicatively augment it with the uid of the operation.

% When we want to lookup an directory entry with a prefix we have three
% cases to consider:

% \begin{enumerate}
%     \item \textbf{There is only one entry for the given prefix}: No conflicts
%     concerns this entry, we can simply return it whatever the ViewIDs are.
%     \item \textbf{There is a prefix conflict and one entry share the same ViewId}:
%     In that case, we assume that the user refers to its own file and resolve it
%     as if there was no conflict.
%     \item \textbf{There is a prefix conflict and no entry has a matching ViewID}:
%     We cannot now for sure what the user referred to, a FQN is required.
% \end{enumerate}

% This resolution also happen when listing a directory. By default,
% when there is no conflict, the user will only see prefix names.

% In case of conflict, a prefix is showed for in conflict entries that share
% the same ViewId of the user otherwise it is
% the FQN. A benefit of this approach is that the user always know when to use
% a shorthand or a fully qualified name.

% To resolve a conflicts, the user only need to use the standard
% FS operations with an FQN or a prefix name. In ElmerFS we didn't need any other
% special mechanism.
% To give an example,
% if a user wants to delete a link named \textit{file1:bob} that is in conflict
% with is own entry \textit{file1} a call to \textit{rm file1:bob}
% is enough and intuitive.

\section{Experimentation and future explorations}

\subsection{Convergence and overhead.}
\subsection{About conditional transactions ?}

\section{Conclusion}



\end{document}
