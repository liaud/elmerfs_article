\documentclass[sigplan, 10pt]{acmart}
\usepackage[utf8]{inputenc}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\settopmatter{printfolios=true}
\begin{document}

\author{Romain Vaillant}
\email{romain.vaillant@scality.com}

\title{ElmerFS: Building a file system using CRDTs}

\begin{abstract}
\end{abstract}

\maketitle


\section{Introduction}

File system is one of the most approachable user interface to organize
and access data that we have today.

With the recent evolutions, the need in an storage capacity and
good response times to distribute and share information across the world has
only increased. Geo-distributed file systems are a key component
to respond to this need.

However, building geo-distributed file systems are notoriously hard and harder
right, it is an old problem (ref WeTransfer, BellLabs, AFS).
Active-Active replication results in conflicts that are hard to resolve.
Serializing critical operations with distributed locking can help but
it is slow and problematic under failure.

Many existing system are either unavailable or incorrect,
or both, under some critical operations, and do not scale well
to an enterprise workload.

This paper address the problem of highly available shared and geo-replicated
distributed hierarchical file systems (FS). We explore the path of
building a distributed file system that is geos-distributed,
highly resilient and scalable by using
\textbf{Conflict Free Replicated Data Types} (CRDTs) i.e. replicated data
structures that don't need any coordination. They are central to build systems
that are highly available, truly concurrent and efficient even
under extreme failure.

Our designs ensure that our file system remain correct and available under
network partition and server crashes. Our experiments shows that ElmerFS can
scale up to 3 geo-distant data centers with 5 node each while being issued
operation fighting for the same resource.

\section{Designing a file system}

\subsection{What a basic file system requires}

Our requirement is to follow the POSIX standard.  We implement
a user-space file system using the FUSE protocol (ref).

A file system supports structural operations operations (to manipulate the file system
structure, permissions and retrieving some information) and content operations
(read/write arbitrary amount of data to a given file).

We use the following terms:

\begin{itemize}
    \item \textbf{A inode}: Is the internal representation of a filesystem
    structure object (a file or a directory or a symbolic link).
    \item \textbf{A link}: A reference to some inode. An inode that is not
    linked may be delete
    \item \textbf{An ino}: A unique id used to reference an inode.
    \item \textbf{A directory entry}: The named and visible representation
    of a link. When a name is used to reference an inode,
    the system resolve internally the name into an ino.
\end{itemize}

The key structure of a file system is its tree hierarchy, the most common
properties that we require are:

\begin{enumerate}
    \item Nodes in this tree are links to inodes. A directory is a container
    of those links.
    \item Directories can have at most one existing link.
    \item All nodes are always reachable from the root directory.
    \item Cycles are possible only through symbolic links.
\end{enumerate}

The file system must also be resilient and concurrent and
have a behavior close to what an user might see in a local filesystem
supporting multiple users.

We whish to support enterprise workloads with concurrent user interactions. Our
expectation are that the application should be able to run up to 5
geo-distanced clusters with a minimum of 5 nodes per cluster.

Each node should be able to provide enough read write performance
for writing and reading documents and images.

No focus have been done on I/O performances, but to adhere to this
workload we require to stay in the range of 1mBytes/s for writes
and more than 10mBytes/s for reads. We don't expect any limit on the file size
apart from the underlying storage capacity.

\subsection{Assumptions and objectives}

We want to leverage CRDTs to develop a file system that is always available
and that provides good response times whatever the network conditions.
It must support active-active configurations (i.e. two geographically distant
clusters can issue read, write and structural operation at the same time without
coordination with each other).

Given the requirements, we want to keep the behavior of the file system as
close as possible to a local file system.

In summary, we want the following properties:

\begin{itemize}
    \item \textbf{Truly concurrent operations}: One way to
    handle concurrency is to serialize operation applied on the relevant objects.
    CRDTs avoids this and allow true concurrency without the need for a consensus.
    \item \textbf{Atomic operation}: POSIX impose that every FS operation
    is atomic.
    \item \textbf{Always available}: CRDT types allows to design system that
    are always available even under network extreme conditions.
    \item \textbf{Active-Active}: Several replica accept
    operations (structural and updates) concurrently and propagate them
    from one another, even after long delays.
    \item \textbf{Least surprise}: We don't want to perform too many actions
    or changes besides what the ones the users does. We want to
    avoid rollbacks or drastic structural changes behind the scene to stay close
    to a local file system behavior.
\end{itemize}

\subsection{Concurrency in a File System}

Under an environnement without coordination, a number of new issues can arise
to preserve the file system invariants. Let's consider some of those
problematic operations and their characteristic..

\subsubsection{Inode creation}

All operations that creates inodes
(\textit{mkdir} for directories and \textit{mknod} for files and symbolic links)
needs to generate an unique id.

This requires to ensure that no replicas can ever generate the same id even
if they can't talk to each other. This problems is often solved with the use
of UUIDs, a 16 bytes number, which have a large enough domain such
that the probability to generate a duplicate is practically null.
However, in POSIX, the requirement for this id is to be a 8 bytes number.

\subsubsection{Named links}

Operations that create or move links (\textit{rename}, \textit{link}
and creation operations) need to consider the case where two users
concurrently want to create a link with an identical name
inside a same directory.

In local file systems or distributed with coordination,
this problem does not exists as those operation are serialized. Serializing
those operation by the use of distributed locking is an option, but that would
lessen the advantages of CRDTs as almost all structural operations would need
coordination.

Additional problems arise if we decide to accept temporary conflicts. In such
cases, we have to inform the user of a conflict in the POSIX interface
which wasn't designed for this kind of situations. Care needs also to be taken
to not impact the current workload of one user from external structural
updates.

\subsubsection{Preserving the FS tree invariants}

By its nature \textit{rename} is an exclusive operation.
As soon as a link is moved, subsequent moves on the same link should fail.
When we decide to not serialize, this operation can create cycles
or unexpected additional links.

For example, without coordination, two users trying to rename
two directory into each other might make each directory be the parent of the other.
Once this situation arise, the concerned directories aren't reachable
from the root directory anymore.

The user can't solve the problem by himself and the application must
decide what must be done. Many distributed file system fails
to handle this situation, leaving the user with an unrecoverable error.

When \textit{rename} is used concurrently on the same source link to a different
destination, if both operations are accepted, it will create one additional
link of the inode. Additionally, for directories, the POSIX interface requires
that directories can only have one unique parent at all time.

\subsubsection{Deletion of inodes}

The \textit{unlink} operation, which removes links, poses problems
in the presence of concurrent operations that can create links.

Consider the simple case where one user \textit{unlink} a file while another
one \textit{link}s it. The inode's state stays correct when the
application will process the link operation from another replicas.

Recycling the inode (its unique id for example) also becomes a problems.
We cannot know when all replicas that once saw this inode
will converge to a state where this inode is not reachable anymore.

Similar issues concerning permissions can be observed. Usually coordination is
needed to ensure that permission change are honored by every replicas at
the time at which they are applied.

\section{System Overview}

ElmerFS is a process that represent a file system replica. Replicas
sharing the same data-center share their storage as one unit and forms
a cluster to answer local user operations. For example, a user issuing a write
at a replica will effectively write to one node of the cluster. Following reads,
will lookup the correct node that contain the relevant content.

When multiple data-centers are involved, a full asynchronous replication takes
place, with each data center holding the most recent view of file system
that it is aware of. With our design, it is possible for this data-center
to become isolated for an arbitrary amount of time. Also, no central authority
is needed. The is not a minimal requirement of node per cluster.

An ElmerFS process is composed of the following layers:

\subsection{The FUSE layer}

The FUSE layer: This is the layer that allow us to interface with the
user. We provide a mount point and answer user requests coming from the
kernel. This layer implements the standard FS operations.

It takes incoming FUSE requests, match them with the correct operation
available in the translation layer and then create the appropriate response.

ElmerFS is multi-threaded and asynchronous, each FUSE request spawns
a single independent task that will run concurrently with the rest of the
system.

\subsection{The persistent CRDT layer}

The persistent layer is responsible to persist CRDT.
In our case, persistence and replication is achieved by AntidoteDB, a
distributed CRDT database.

It is a key-value store, where each value is a CRDT. AntidoteDB provides
us a library of needed CRDT types.
This means that the designer must map structures to the appropriate CRDT types
and ensure that he can express the application's operations
with the set of operation offered by the chosen CRDT types.

AntidoteDB implements for us the replication protocol and the persistence and
act as the only source of truth for our state.

From the library of available CRDTs, we mainly use three kind:
register which are blobs of data with Last Writer Win (LWReg) semantics and
Remove Win Maps/Sets (RWMap and RWSet) that favor remove operations
in case of conflicting adds.

\subsection{The model Layer}

The model layer is where we map CRDT to the in memory
state of the application. We split ElmerFS state into
four mains entities: inodes, symlink paths blocks, directories entries.

Inodes which store POSIX metadata information of any inode in the file system
are stored as RWMap mapping each attribute.

Files in ElmerFS are sparse, gaps are allowed. We use LWReg
of fixed size to store arbitrary user content. We only allocate blocks
for ranges that have been written.

Symbolic links are just a special case of arbitrary file content. They
store the actual target path.

Folders are stored as RWSets of ino and names.
The file system hierarchy is implicit, parent folder are contains their child
directories and child directory keeps a pointer to their parent through the
special ".." file.

\subsection{The Translation Layer}

This translation layer is where we maps
the application operations to the correct sequence of CRDT operations.
This is where the main logic of the file system happens.

This layer also ensure that FS operation are atomics. This achieved "for free"
by the use of the interactive transactions provided by AntidoteDB. Each high
level operation wrapped inside a single transaction. In these transaction
we are free to perform any number of read or updates on multiple objects.
Updates will all be visible after the final commit.

\section{The translation of high level operations}

CRDT ensure Strong Eventual Consistency (SEC) (ref) but do
not ensure that the application invariants remains correct. The challenge
is to keep those invariant always correct under any sequence of
operation.

When translating our high level operations, we had to consider problems stated
in section 2 and chose the proper CRDT types, what metadata needs to be stored
and what sequence of CRDT operations needs to be issued to remain correct.

\subsection{Generating the inode number}

As stated in section 2.3.1, POSIX requirements on the inode number are hard to
satisfy in a system without coordination.

When \textit{mkdir} and \textit{mknod} are called, we need to find an unique id
for the created inode. To solve this problem, we use a global
counter whose access is serialized through a distributed lock.
To limit this contention point, each time that we access the global counter
we reserve a range of ino that we will then consume locally.

We still haven't found a proper CRDT that provides an operation to generate
unique identifiers across replicas.

Note that we also don't recycle ino of deleted inode,
we can't assert that all replicas have converged to a state where the ino
is not used anymore.

\subsection{Ensuring deletion}

To ensure that delete operations are honored, we had to chose RWMap and sets.
Favoring concurrent remove operations in our file systems means
that when a \textit{unlink} is concurrent with a \textit{link}
or a \textit{rename}, the \textit{unlink} will prevail
when replicas will converge.

RWMap are also easier to keep correct on the application side. We can only
update some key of the map and issue a remove operation on all possible
existing key (which is a finite amount when we are mapping structures) to ensure
that the map will never converge to a partial state.

This address the some of the issues mentioned in section 2.3.4. However, we
believe a better and more general solution would be to allow transaction to be
paired with deterministic conditions to decide if it should be committed or
aborted when they are concurrent with other transaction concerning the same
objects.

\subsection{Interfacing with the user}

In ElmerFS, we allow names conflict to happens and we expect the user solves
those conflict using FS operations that are available to him.

To be able to distinguish between two inodes sharing a same name under a same
parent directory, we use an additional unique identifier, the ViewID.
Apart from being unique, there is no particular requirement for this
identifier. We chose to use the file system user id (uid) and we expect that a
user won't issue operations from two different processes.

Each time an user creates link, we not only store the name and the ino of the
link but also the ViewID of the user that created it. Because we use RWSet
for directories, entries that would have been previously considered
the same are now distinct.

To interface with the user, we use the concept of partial and Fully Qualified
Names (FQNs). Partial names are how the user named the link, FQN are partial
names concatenated with the ViewID. Since the ViewID is unique,
we know that all visible link are uniquely identifiable.

In a situation where no conflict exists, we always show partial names. When
there is a conflict, we only show FQN of entries that have a ViewID that
doesn't match the one used by the user. Showing only FQN when the ViewID
mismatch allow the user to continue to work by name on the file while other
conflicting operation might be merged. We always favor the ViewID of the user that issued the request

For example, in a situation with two user, bob and alice where both have
created a file name "us". If bob tries to list the directory, he will see two
files: "us" and "us:alice". To him "us" is still its own file and we do not
lose any content when the CRDTs are merged.

This solve the issue on named links described in section 2.3.2.

% \subsection{A common base}

% We won't describe in details how ElmerFS identify its entities,
% it simply computes a composite key from a kind
% (inode, block, directory, ...) and an ino.

% The ino generation is also very simple. Each instance reserves a predetermined
% amount of ino beforehand and keeps them in memory. To ensure that multiple
% instances do not produce the same ino, we have to use a lock. A counter CRDT
% does not work for this (we have to read the value, not just incrementing it). In fact
% this is the only place where a mandatory lock is used.
% A sequence CRDT (\textbf{TODO: which one}) might be a solution but aren't available in
% AntidoteDB.

% A consequence of this approach is that inos are never recycled. We can never
% be certain when a CRDT will never be used again as we allow divergence for an
% not constrained amount of time. However, for this particular case, 64bit is a
% large enough domain for this problem to be problematic for most common use
% cases.

% Our inode contains all metadata information that are common in POSIX:
% permissions, owner, change, modification and access
% time. As described above, we represent them using a RWMap.
% We made the decision not to store data information of the file and the
% symbolic link path alongside the metadata.

% As only inodes represent the existence of entities, this separation leads to
% potential leaks if some inode is deleted in one site and written to in another.
% However, it is more a limitation of the API of AntidoteDB than CRDTs which
% does not support partial read or update.

% \subsection{The heart of a file system interface}

% To represent the tree hierarchy, it is easy to see that just applying CRDTs merging semantics
% without care can easily break the tree constraints.
% There is active research (ref?) to provide a Tree CRDT with add, remove and move operations.
% In ElmerFS we didn't implement one of such CRDT that would have ease our
% development. We explored the path of only using RWMaps.

% Even though we still have found any simple solution to prevent cycle apart from
% keeping track of all rename operations and cancelling the
% one in conflict in a deterministic way, we still manage to handle
% most of the concurrent tree operations gracefully without distributed locks.
% This solution isn't perfect, our choice to prevent cycles would be costly at
% the application level.

% \section{Embracing the CRDT merging semantics}

% To handle most conflicting operations with only common
% CRDTs, ElmerFS introduces the concept of an owner of each operation and
% each owner has its own view of the current file system.

% \subsection{Some problematic cases}

% First, let's consider some problematic cases of the rename operation.
% On a POSIX file system, a rename operation is expected to be exclusive
% with respect to the renamed inodes.

% % confusing
% In a concurrent scenario, the first problematic situation
% arises when two users try to rename distinct inodes to the same target location with the same name.
% In that case, CRDT maps merging semantics will only keep one operation depending
% in which order they arrived. In all cases one link will be lost and the inode
% might become unreachable if the was the only link to that was referencing it.

% Moreover, those name conflicts can also happen with all operations that
% can create links: link, mkdir, mknod\dots

% The second situation happens when a unique reference is renamed
% by two users at the same time to two different locations. The standard behavior
% is that one of the operation will fail. This poses two new problems: In a completely
% asynchronous situation, how do we ensure that the state converge something that
% is meaningful to the user and how the reference count can be handled
% with an operation that semantically does not affect it ?

% \subsection{Keeping conflicts where they make sense}

% In ElmerFS, we solve these problems by using an unique value that we call the
% ViewID. Th
% Another solution would have been to tie a unique id to each process
% but we find that the mapping between the user and its actions is more
% straightforward in the former solution.

% In our application we use a RWSet CRDT to represent directories.
% We depart from standard file system by not only storing a pair of ino
% and name to represent a link but also by storing the ViewID alongside this
% original pair.

% If we take the previous concurrent case where two users tries to rename two
% inode to the same location, with the ViewID associated with each name, we
% ensure that we either don't have a conflict (no tuple in the set
% share a same name entry) or that we keep all succeeded operations
% in our application state: there might be two identical name in the set, but they
% will have a different ViewID.

% By adding uniqueness to our directory entries, we allow precise conflicts
% to bubble up to the application logic when they make sense.
% When two user try to create a new file inside a same directory, we don't
% really want one operation to fail afterward. As soon as the user created the
% file and started writing it, it is crucial for the file system to keep this
% file. Conflicts here allow us to choose how to present and how to manage
% conflict without designing a specific CRDTs.

% \subsection{Linking and reference counting}

% By using the same mechanism we can also keep track of the number of reference
% of each inode where a counter CRDT would obviously fall short. One can imagine two users
% renaming the same file to a different location. In ElmerFS, we accept this kind
% of operation but it means that this operation is similar to creating
% an additional link.
% We use a set CRDT to store all links of an inode.
% Inside a transaction,
% we issue a remove operation of the previous link and an add operation of the new link. The remove operation will contains the exact tuple matched
% (with the ViewID that created the link), When the CRDT converges, it receives
% two identical remove operations, and two distinct add operations, leading to the right reference count.

% Directories have the additional requirements that they can only have one unique
% parent in the file system. To preserve this invariant, we also use a register
% with last writer win semantics, to decide which reference is valid. Note that
% we could also allow multiple link of a directories,
% this is only a limitation of the POSIX interface.

% \subsection{Interfacing with the user}

% Interfacing with the user in case of non standard FS behaviors requires care.
% While we solved some issues with the renames and links operation, we still
% need to show to the user that the conflict exists and that it needs to be
% resolved. Since in a FS the user is expected to reference entities by name,
% we kept this idiom to resolve conflict.

% In ElmerFS, a name is either a shorthand or a fully qualified (FQN).
% What we call a FQN is a standard POSIX name (the prefix) and
% the ViewID delimited by a special character.
% For example, in ElmerFS we use the uid for convenience and
% we allow user to specify the username instead of the id directly:
% the FQN of \textit{file1} for the user
% \textit{bob} is \textit{file1:bob}. When only the filename is specified, we
% implicatively augment it with the uid of the operation.

% When we want to lookup an directory entry with a prefix we have three
% cases to consider:

% \begin{enumerate}
%     \item \textbf{There is only one entry for the given prefix}: No conflicts
%     concerns this entry, we can simply return it whatever the ViewIDs are.
%     \item \textbf{There is a prefix conflict and one entry share the same ViewId}:
%     In that case, we assume that the user refers to its own file and resolve it
%     as if there was no conflict.
%     \item \textbf{There is a prefix conflict and no entry has a matching ViewID}:
%     We cannot now for sure what the user referred to, a FQN is required.
% \end{enumerate}

% This resolution also happen when listing a directory. By default,
% when there is no conflict, the user will only see prefix names.

% In case of conflict, a prefix is showed for in conflict entries that share
% the same ViewId of the user otherwise it is
% the FQN. A benefit of this approach is that the user always know when to use
% a shorthand or a fully qualified name.

% To resolve a conflicts, the user only need to use the standard
% FS operations with an FQN or a prefix name. In ElmerFS we didn't need any other
% special mechanism.
% To give an example,
% if a user wants to delete a link named \textit{file1:bob} that is in conflict
% with is own entry \textit{file1} a call to \textit{rm file1:bob}
% is enough and intuitive.

\section{Experimentation and future explorations}

\subsection{Convergence and overhead.}
\subsection{About conditional transactions ?}

\section{Conclusion}



\end{document}
